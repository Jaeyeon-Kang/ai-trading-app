"""
Celery beat Ïä§ÏºÄÏ§ÑÎü¨
15-30Ï¥à Ï£ºÍ∏∞Î°ú ÏãúÍ∑∏ÎÑê ÏÉùÏÑ± Î∞è Í±∞Îûò Ïã§Ìñâ
"""
from datetime import datetime, timedelta
import hashlib
import json
import logging
import os
import time
from typing import Dict, List, Optional
import urllib.parse as _urlparse

import psycopg2
import redis

from celery import Celery
from celery.schedules import crontab
from celery.signals import beat_init, task_prerun, worker_process_init, worker_ready

from app.config import settings, get_signal_cutoffs, sanitize_cutoffs_in_redis

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Celery Ïï± ÏÉùÏÑ±
celery_app = Celery(
    "trading_bot",
    broker=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
    backend=os.getenv("REDIS_URL", "redis://localhost:6379/0")
)

# ÏÑ§Ï†ï
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="Asia/Seoul",  # KST
    enable_utc=True,        # UTC Ïú†ÏßÄ + ÌÉÄÏûÑÏ°¥-aware Ïä§ÏºÄÏ§Ñ OK
    task_track_started=True,
    task_time_limit=30 * 60,  # 30Î∂Ñ
    task_soft_time_limit=25 * 60,  # 25Î∂Ñ
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# Ïä§ÏºÄÏ§Ñ ÏÑ§Ï†ï
celery_app.conf.beat_schedule = {
    # 15Ï¥àÎßàÎã§ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ (E2E)
    "pipeline-e2e": {
        "task": "app.jobs.scheduler.pipeline_e2e",
        "schedule": 15.0,  # 15Ï¥à
    },
    # 15Ï¥àÎßàÎã§ ÏãúÍ∑∏ÎÑê ÏÉùÏÑ±
    "generate-signals": {
        "task": "app.jobs.scheduler.generate_signals",
        "schedule": 15.0,  # 15Ï¥à
    },
    # 30Ï¥àÎßàÎã§ ÏãúÏÑ∏ ÏóÖÎç∞Ïù¥Ìä∏
    "update-quotes": {
        "task": "app.jobs.scheduler.update_quotes",
        "schedule": 30.0,  # 30Ï¥à
    },
    # 1Î∂ÑÎßàÎã§ EDGAR Ïä§Ï∫î
    "scan-edgar": {
        "task": "app.jobs.scheduler.scan_edgar",
        "schedule": 60.0,  # 1Î∂Ñ
    },
    # 5Î∂ÑÎßàÎã§ Î¶¨Ïä§ÌÅ¨ Ï≤¥ÌÅ¨
    "check-risk": {
        "task": "app.jobs.scheduler.check_risk",
        "schedule": 300.0,  # 5Î∂Ñ
    },
    # Îß§Ïùº ÏûêÏ†ïÏóê ÏùºÏùº Î¶¨ÏÖã
    "daily-reset": {
        "task": "app.jobs.scheduler.daily_reset",
        "schedule": crontab(hour=0, minute=0),  # Îß§Ïùº 00:00
    },
    # Îß§Ïùº 06:10 KSTÏóê ÏùºÏùº Î¶¨Ìè¨Ìä∏ (KST = UTC+9, 06:10 KST = 21:10 UTC Ï†ÑÎÇ†)
    "daily-report": {
        "task": "app.jobs.scheduler.daily_report",
        "schedule": crontab(hour=21, minute=10),  # Îß§Ïùº 21:10 UTC = 06:10 KST
        "args": [False, True],  # force=False, post=True (Ïä¨ÎûôÏúºÎ°ú Î≥¥ÎÇ¥Í∏∞)
    },
    # 5Ï¥àÎßàÎã§ EDGAR Ïä§Ìä∏Î¶º ÏàòÏßë ‚Üí DB Ï†ÅÏû¨ (dedupeÎäî DB UNIQUEÏôÄ ON CONFLICTÎ°ú Î≥¥Í∞ï)
    "ingest-edgar": {
        "task": "app.jobs.scheduler.ingest_edgar_stream",
        "schedule": 5.0,
    },
    # 15Î∂ÑÎßàÎã§ Ïú†ÎãàÎ≤ÑÏä§ Ïû¨Ï†ÅÏö© (Í∂åÏû• Î∂ÑÎ¶¨)
    "refresh-universe": {
        "task": "app.jobs.scheduler.refresh_universe",
        "schedule": 900.0,
    },
    # Îß§Ïùº 05:55 KST Ï†ÅÏùëÌòï Ïª∑Ïò§ÌîÑ Í∞±Ïã† (Î¶¨Ìè¨Ìä∏ ÏßÅÏ†Ñ)
    "adaptive-cutoff": {
        "task": "app.jobs.scheduler.adaptive_cutoff",
        "schedule": crontab(hour=20, minute=55),  # 05:55 KST = 20:55 UTC Ï†ÑÎÇ†
    },
}

# Ï†ÑÏó≠ Î≥ÄÏàò (Ïã§Ï†úÎ°úÎäî ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ ÏÇ¨Ïö©)
trading_components = {
    "quotes_ingestor": None,
    "edgar_scanner": None,
    "regime_detector": None,
    "tech_score_engine": None,
    "llm_engine": None,
    "signal_mixer": None,
    "risk_engine": None,
    "paper_ledger": None,
    "redis_streams": None,
    "slack_bot": None,
    "stream_consumer": None,
    "db_connection": None,
}

def _parse_redis_url(url: str) -> tuple[str, int, int]:
    try:
        parsed = _urlparse.urlparse(url)
        host = parsed.hostname or "redis"
        port = int(parsed.port or 6379)
        db = int((parsed.path or "/0").lstrip("/") or 0)
        return host, port, db
    except Exception:
        return "redis", 6379, 0

def _autoinit_components_if_enabled() -> None:
    """Best-effort Ïª¥Ìè¨ÎÑåÌä∏ ÏûêÎèô Ï¥àÍ∏∞Ìôî (ÏõåÏª§/ÎπÑÌä∏ ÌîÑÎ°úÏÑ∏Ïä§ Í∏∞Îèô Ïãú).
    ÌïÑÏàò Íµ¨ÏÑ±ÎßåÏù¥ÎùºÎèÑ Ï§ÄÎπÑÌï¥ components_not_ready Ïä§ÌÇµÏùÑ Ï§ÑÏù∏Îã§.
    """
    if os.getenv("AUTO_INIT_COMPONENTS", "true").lower() not in ("1","true","yes","on"):  # opt-out
        return
    try:
        from app.io.quotes_delayed import DelayedQuotesIngestor
        from app.engine.regime import RegimeDetector
        from app.engine.techscore import TechScoreEngine
        from app.engine.mixer import SignalMixer
        from app.engine.risk import RiskEngine
        from app.adapters.paper_ledger import PaperLedger
        from app.io.streams import RedisStreams, StreamConsumer
        from app.engine.llm_insight import LLMInsightEngine
        from app.io.slack_bot import SlackBot
    except Exception as e:
        logger.warning(f"Ïª¥Ìè¨ÎÑåÌä∏ ÏûÑÌè¨Ìä∏ Ïã§Ìå®: {e}")
        return

    components: Dict[str, object] = {}

    # Quotes ingestor
    try:
        components["quotes_ingestor"] = DelayedQuotesIngestor()
        # ÏõåÎ∞çÏóÖÏúºÎ°ú Îπà Î≤ÑÌçº Î∞©ÏßÄ
        components["quotes_ingestor"].warmup_backfill()  # type: ignore
        logger.info("ÎîúÎ†àÏù¥Îìú Quotes Ïù∏Ï†úÏä§ÌÑ∞ ÏõåÎ∞çÏóÖ ÏôÑÎ£å")
    except Exception as e:
        logger.warning(f"Quotes Ïù∏Ï†úÏä§ÌÑ∞ Ï§ÄÎπÑ Ïã§Ìå®: {e}")

    # Regime/Tech/Mixer
    try:
        components["regime_detector"] = RegimeDetector()
    except Exception as e:
        logger.warning(f"RegimeDetector Ï§ÄÎπÑ Ïã§Ìå®: {e}")
    try:
        components["tech_score_engine"] = TechScoreEngine()
    except Exception as e:
        logger.warning(f"TechScoreEngine Ï§ÄÎπÑ Ïã§Ìå®: {e}")
    try:
        thr = settings.MIXER_THRESHOLD
        components["signal_mixer"] = SignalMixer(buy_threshold=thr, sell_threshold=-thr)
    except Exception as e:
        logger.warning(f"SignalMixer Ï§ÄÎπÑ Ïã§Ìå®: {e}")

    # Risk/Paper ledger
    try:
        init_cap = float(os.getenv("INITIAL_CAPITAL", "1000000"))
        components["risk_engine"] = RiskEngine(initial_capital=init_cap)
    except Exception as e:
        logger.warning(f"RiskEngine Ï§ÄÎπÑ Ïã§Ìå®: {e}")
    try:
        components["paper_ledger"] = PaperLedger(initial_cash=float(os.getenv("INITIAL_CAPITAL", "1000000")))
    except Exception as e:
        logger.warning(f"PaperLedger Ï§ÄÎπÑ Ïã§Ìå®: {e}")

    # Redis streams / consumer
    try:
        rurl = os.getenv("REDIS_URL", "redis://redis:6379/0")
        host, port, db = _parse_redis_url(rurl)
        rs = RedisStreams(host=host, port=port, db=db)
        components["redis_streams"] = rs
        components["stream_consumer"] = StreamConsumer(rs)
    except Exception as e:
        logger.warning(f"Redis Streams Ï§ÄÎπÑ Ïã§Ìå®: {e}")

    # Slack bot (optional)
    try:
        token = os.getenv("SLACK_BOT_TOKEN")
        channel = os.getenv("SLACK_CHANNEL_ID") or os.getenv("SLACK_CHANNEL", "#trading-signals")
        logger.info(f"üîç [DEBUG] SlackBot ÌôòÍ≤ΩÎ≥ÄÏàò: token={'***' if token else None}, channel_id={os.getenv('SLACK_CHANNEL_ID')}, channel_fallback={os.getenv('SLACK_CHANNEL')}")
        logger.info(f"üîç [DEBUG] SlackBot ÏµúÏ¢Ö Ï±ÑÎÑê: {channel}")
        if token:
            components["slack_bot"] = SlackBot(token=token, channel=channel)
            logger.info(f"Slack Î¥á Ï§ÄÎπÑ: Ï±ÑÎÑê {channel}")
        else:
            logger.warning("Slack ÌÜ†ÌÅ∞ ÏóÜÏùå - Ïä¨Îûô ÎπÑÌôúÏÑ±")
    except Exception as e:
        logger.warning(f"SlackBot Ï§ÄÎπÑ Ïã§Ìå®: {e}")

    # LLM (optional)
    try:
        llm = LLMInsightEngine()
        if components.get("slack_bot"):
            llm.set_slack_bot(components["slack_bot"])  # type: ignore
        components["llm_engine"] = llm
    except Exception as e:
        logger.warning(f"LLM ÏóîÏßÑ Ï§ÄÎπÑ Ïã§Ìå®: {e}")

    if components:
        try:
            initialize_components(components)  # Í∏∞Ï°¥ Ìó¨ÌçºÎ°ú Ï£ºÏûÖ
        except Exception as e:
            logger.warning(f"Ïª¥Ìè¨ÎÑåÌä∏ Ï£ºÏûÖ Ïã§Ìå®: {e}")

# Celery Ïã†Ìò∏Ïóê ÏûêÎèô Ï¥àÍ∏∞Ìôî Ïó∞Í≤∞
@worker_ready.connect
def _on_worker_ready(sender=None, **kwargs):
    _autoinit_components_if_enabled()
    # Ïª∑Ïò§ÌîÑ Ï†ïÌôî Î∞è Î°úÍπÖ
    result = sanitize_cutoffs_in_redis()
    if result:
        logger.info(f"Ïª∑Ïò§ÌîÑ Î°úÎìúÎê®: RTH={result['rth']:.3f}, EXT={result['ext']:.3f}")
    else:
        rth, ext = get_signal_cutoffs()
        logger.info(f"Ïª∑Ïò§ÌîÑ Î°úÎìúÎê®: RTH={rth:.3f}, EXT={ext:.3f}")

@beat_init.connect
def _on_beat_init(sender=None, **kwargs):
    _autoinit_components_if_enabled()
    # Ïª∑Ïò§ÌîÑ Ï†ïÌôî Î∞è Î°úÍπÖ
    result = sanitize_cutoffs_in_redis()
    if result:
        logger.info(f"Ïª∑Ïò§ÌîÑ Î°úÎìúÎê®: RTH={result['rth']:.3f}, EXT={result['ext']:.3f}")
    else:
        rth, ext = get_signal_cutoffs()
        logger.info(f"Ïª∑Ïò§ÌîÑ Î°úÎìúÎê®: RTH={rth:.3f}, EXT={ext:.3f}")

# Í∞Å ÏõåÏª§ ÌîÑÎ°úÏÑ∏Ïä§(prefork)ÏóêÏÑú Ìïú Î≤àÏî© Ï¥àÍ∏∞Ìôî
@worker_process_init.connect
def _on_worker_process_init(sender=None, **kwargs):
    _autoinit_components_if_enabled()
    # Ïª∑Ïò§ÌîÑ Ï†ïÌôî Î∞è Î°úÍπÖ
    result = sanitize_cutoffs_in_redis()
    if result:
        logger.info(f"Ïª∑Ïò§ÌîÑ Î°úÎìúÎê®: RTH={result['rth']:.3f}, EXT={result['ext']:.3f}")
    else:
        rth, ext = get_signal_cutoffs()
        logger.info(f"Ïª∑Ïò§ÌîÑ Î°úÎìúÎê®: RTH={rth:.3f}, EXT={ext:.3f}")

# ÌÉúÏä§ÌÅ¨ ÏßÅÏ†ÑÏóêÎèÑ ÌïÑÏàò Ïª¥Ìè¨ÎÑåÌä∏Í∞Ä ÏóÜÏúºÎ©¥ ÎßàÏßÄÎßâ ÏãúÎèÑ
@task_prerun.connect
def _ensure_components_before_task(task=None, **kwargs):
    try:
        required_keys = [
            "quotes_ingestor",
            "regime_detector",
            "tech_score_engine",
            "signal_mixer",
        ]
        missing = [k for k in required_keys if not trading_components.get(k)]
        if missing:
            logger.warning(f"ÌïÑÏàò Ïª¥Ìè¨ÎÑåÌä∏ ÎØ∏Ï§ÄÎπÑ(prerun): {missing} ‚Üí ÏûêÎèô Ï¥àÍ∏∞Ìôî ÏãúÎèÑ")
            _autoinit_components_if_enabled()
    except Exception as e:
        logger.warning(f"prerun ÏûêÎèô Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")

@celery_app.task(bind=True, name="app.jobs.scheduler.pipeline_e2e")
def pipeline_e2e(self):
    """E2E ÌååÏù¥ÌîÑÎùºÏù∏: EDGAR Ïù¥Î≤§Ìä∏ ‚Üí LLM ‚Üí Î†àÏßê ‚Üí ÎØπÏÑú ‚Üí DB ‚Üí Slack"""
    try:
        start_time = time.time()
        logger.info("E2E ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÏûë")
        
        # Ïª¥Ìè¨ÎÑåÌä∏ ÌôïÏù∏ (ÌïÑÏàò ÏµúÏÜå Íµ¨ÏÑ±Îßå Í∞ïÏ†ú, LLMÏùÄ ÏÑ†ÌÉù)
        if not all([
            trading_components["stream_consumer"],
            trading_components["regime_detector"],
            trading_components["signal_mixer"],
            trading_components["slack_bot"]
        ]):
            logger.warning("ÌïÑÏàò Ïª¥Ìè¨ÎÑåÌä∏ ÎØ∏Ï§ÄÎπÑ(stream_consumer/regime_detector/signal_mixer/slack_bot)")
            return {"status": "skipped", "reason": "components_not_ready"}
        
        stream_consumer = trading_components["stream_consumer"]
        llm_engine = trading_components["llm_engine"]
        regime_detector = trading_components["regime_detector"]
        signal_mixer = trading_components["signal_mixer"]
        slack_bot = trading_components["slack_bot"]
        
        signals_processed = 0
        
        # 1. EDGAR Ïù¥Î≤§Ìä∏ ÏÜåÎπÑ
        edgar_events = stream_consumer.consume_edgar_events(count=5, block_ms=100)
        
        for event in edgar_events:
            try:
                ticker = event.data.get("ticker")
                if not ticker:
                    continue
                
                # 2. LLM Î∂ÑÏÑù (EDGAR Ïù¥Î≤§Ìä∏Ïù¥ÎØÄÎ°ú Ï°∞Í±¥ Ï∂©Ï°±)
                llm_insight = None
                if llm_engine:
                    text = event.data.get("snippet_text", "")
                    url = event.data.get("url", "")
                    llm_insight = llm_engine.analyze_text(text, url, edgar_event=True)
                
                # 3. ÏãúÏÑ∏ Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞ (Í∞ÑÎã®Ìïú Î™®Ïùò Îç∞Ïù¥ÌÑ∞)
                candles = get_mock_candles(ticker)
                indicators = get_mock_indicators(ticker)
                
                # 4. Î†àÏßê Í∞êÏßÄ
                regime_result = regime_detector.detect_regime(candles)
                
                # 5. Í∏∞Ïà†Ï†Å Ï†êÏàò Í≥ÑÏÇ∞
                tech_score = get_mock_tech_score(ticker)
                
                # 6. ÏãúÍ∑∏ÎÑê ÎØπÏã±
                current_price = 150.0  # Î™®Ïùò Í∞ÄÍ≤©
                signal = signal_mixer.mix_signals(
                    ticker=ticker,
                    regime_result=regime_result,
                    tech_score=tech_score,
                    llm_insight=llm_insight,
                    edgar_filing=event.data,
                    current_price=current_price
                )
                
                if signal:
                    # Î©îÌÉÄÏóê ÏÑ∏ÏÖò/ÌíàÏßà ÏßÄÌëú Ïã¨Í∏∞ ‚Üí Slack Ìó§Îçî ÎùºÎ≤®Ïö©
                    if not signal.meta:
                        signal.meta = {}
                    session_label = _session_label()
                    signal.meta["session"] = session_label
                    # ÏïàÏ†Ñ Í∞ÄÎìú: ÎØ∏Ï†ïÏùò Î≥ÄÏàò Ï≤òÎ¶¨
                    signal.meta.setdefault("spread_bp", 0.0)
                    signal.meta.setdefault("dollar_vol_5m", 0.0)
                    # 7. DBÏóê Ï†ÄÏû•
                    if trading_components.get("db_connection"):
                        signal_mixer.save_signal_to_db(signal, trading_components["db_connection"])
                    
                    # 8. Slack ÏïåÎ¶º
                    if slack_bot:
                        slack_message = format_slack_message(signal)
                        slack_bot.send_message(slack_message)
                    
                    # 9. Î©îÏãúÏßÄ ACK
                    stream_consumer.acknowledge("news.edgar", event.message_id)
                    
                    signals_processed += 1
                    logger.info(f"ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å: {ticker} {signal.signal_type.value}")
                
            except Exception as e:
                logger.error(f"Ïù¥Î≤§Ìä∏ Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
                continue
        
        execution_time = time.time() - start_time
        logger.info(f"E2E ÌååÏù¥ÌîÑÎùºÏù∏ ÏôÑÎ£å: {signals_processed}Í∞ú, {execution_time:.2f}Ï¥à")
        
        return {
            "status": "success",
            "signals_processed": signals_processed,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"E2E ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìå®: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

def get_mock_candles(ticker: str) -> List:
    """Î™®Ïùò Ï∫îÎì§ Îç∞Ïù¥ÌÑ∞"""
    # Ïã§Ï†úÎ°úÎäî quotes_ingestorÏóêÏÑú Í∞ÄÏ†∏Ïò¥
    return []

def get_mock_indicators(ticker: str) -> Dict:
    """Î™®Ïùò Í∏∞Ïà†Ï†Å ÏßÄÌëú"""
    # Ïã§Ï†úÎ°úÎäî quotes_ingestorÏóêÏÑú Í∞ÄÏ†∏Ïò¥
    return {
        "adx": 25.0,
        "ema_20": 150.0,
        "ema_50": 148.0,
        "rsi": 65.0,
        "volume_spike": 0.3,
        "price_change_5m": 0.02,
        "realized_volatility": 0.03,
        "current_price": 150.0,
        "vwap": 149.5,
        "vwap_deviation": 0.003,
        "macd": 0.5,
        "macd_signal": 0.3,
        "bb_position": 0.6
    }

def get_mock_tech_score(ticker: str):
    """Î™®Ïùò Í∏∞Ïà†Ï†Å Ï†êÏàò"""
    # Ïã§Ï†úÎ°úÎäî tech_score_engineÏóêÏÑú Í≥ÑÏÇ∞
    from app.engine.techscore import TechScoreResult
    return TechScoreResult(
        score=0.7,
        components={
            "ema": 0.8,
            "macd": 0.7,
            "rsi": 0.6,
            "vwap": 0.7,
        },
        timestamp=datetime.now()
    )

def format_slack_message(signal) -> Dict:
    """Slack Î©îÏãúÏßÄ Ìè¨Îß∑"""
    return {
        "channel": "C099CQP8CJ3",  # üîß Î™ÖÏãúÏ†ÅÏúºÎ°ú Ï±ÑÎÑê ID ÏßÄÏ†ï (channel_not_found Ïò§Î•ò Ìï¥Í≤∞)
        "text": (
            f"{signal.ticker} | Î†àÏßê {signal.regime.upper()}({signal.confidence:.2f}) | "
            f"Ï†êÏàò {signal.score:+.2f} {'Î°±' if signal.signal_type.value == 'long' else 'Ïàè'}"
        )
    }

@celery_app.task(bind=True, name="app.jobs.scheduler.generate_signals")
def generate_signals(self):
    """ÏãúÍ∑∏ÎÑê ÏÉùÏÑ± ÏûëÏóÖ"""
    try:
        start_time = time.time()
        logger.info("ÏãúÍ∑∏ÎÑê ÏÉùÏÑ± ÏãúÏûë")
        
        # ÌïÑÏàò ÏµúÏÜå Ïª¥Ìè¨ÎÑåÌä∏ ÌôïÏù∏: Ïä§Ï∫òÌîÑ Í≤ΩÎ°úÎßåÏù¥ÎùºÎèÑ ÎèåÎ¶¥ Ïàò ÏûàÍ≤å ÏµúÏÜå depsÎßå Í∞ïÏ†ú
        # 1) quotes_ingestor ÌïÑÏàò. ÏóÜÏúºÎ©¥ ÌòÑ ÏûêÎ¶¨ÏóêÏÑú ÏÉùÏÑ± ÏãúÎèÑ
        if not trading_components.get("quotes_ingestor"):
            logger.warning("ÌïÑÏàò Ïª¥Ìè¨ÎÑåÌä∏ ÎØ∏Ï§ÄÎπÑ: ['quotes_ingestor'] ‚Üí Î°úÏª¨ ÏÉùÏÑ± ÏãúÎèÑ")
            try:
                from app.io.quotes_delayed import DelayedQuotesIngestor
                trading_components["quotes_ingestor"] = DelayedQuotesIngestor()
                try:
                    trading_components["quotes_ingestor"].warmup_backfill()  # type: ignore
                except Exception:
                    pass
            except Exception as e:
                logger.warning(f"quotes_ingestor ÏÉùÏÑ± Ïã§Ìå®: {e}")
        if not trading_components.get("quotes_ingestor"):
            return {"status": "skipped", "reason": "quotes_ingestor_not_ready"}
        # 2) signal_mixer ÏóÜÏúºÎ©¥ ÌòÑ ÏûêÎ¶¨ÏóêÏÑú Í∏∞Î≥∏Í∞íÏúºÎ°ú ÏÉùÏÑ± (Ïä§Ï∫òÌîÑ Í≤ΩÎ°úÏö©)
        if not trading_components.get("signal_mixer"):
            try:
                from app.engine.mixer import SignalMixer
                thr = settings.MIXER_THRESHOLD
                trading_components["signal_mixer"] = SignalMixer(buy_threshold=thr, sell_threshold=-thr)
                logger.warning("signal_mixer Î°úÏª¨ ÏÉùÏÑ±")
            except Exception as e:
                logger.warning(f"signal_mixer ÏÉùÏÑ± Ïã§Ìå®: {e}")
        # 3) redis_streams ÏóÜÏúºÎ©¥ ÌòÑ ÏûêÎ¶¨ÏóêÏÑú ÏÉùÏÑ± (Î∞úÌñâÏö©)
        if not trading_components.get("redis_streams"):
            try:
                from app.io.streams import RedisStreams
                rurl = os.getenv("REDIS_URL", "redis://redis:6379/0")
                host, port, db = _parse_redis_url(rurl)
                trading_components["redis_streams"] = RedisStreams(host=host, port=port, db=db)
            except Exception as e:
                logger.warning(f"redis_streams ÏÉùÏÑ± Ïã§Ìå®: {e}")
        
        quotes_ingestor = trading_components["quotes_ingestor"]
        edgar_scanner = trading_components["edgar_scanner"]
        regime_detector = trading_components["regime_detector"]
        tech_score_engine = trading_components["tech_score_engine"]
        llm_engine = trading_components["llm_engine"]
        signal_mixer = trading_components["signal_mixer"]
        redis_streams = trading_components["redis_streams"]
        slack_bot = trading_components.get("slack_bot")
        
        # üîç DEBUG: Slack bot ÏÉÅÌÉú ÌôïÏù∏
        logger.info(f"üîç [DEBUG] slack_bot Ï°¥Ïû¨ Ïó¨Î∂Ä: {slack_bot is not None}")
        if slack_bot:
            logger.info(f"üîç [DEBUG] slack_bot ÌÉÄÏûÖ: {type(slack_bot)}")
            logger.info(f"üîç [DEBUG] slack_bot Ï±ÑÎÑê: {getattr(slack_bot, 'default_channel', 'N/A')}")
        else:
            logger.warning(f"üîç [DEBUG] slack_botÏù¥ NoneÏûÑ! trading_components ÌÇ§: {list(trading_components.keys())}")
        
        signals_generated = 0
        
        # Í∞Å Ï¢ÖÎ™©Î≥ÑÎ°ú ÏãúÍ∑∏ÎÑê ÏÉùÏÑ±
        # Ïú†ÎãàÎ≤ÑÏä§ ÎèôÏ†Å Ï†ÅÏö© (Redis union: core + external + watchlist)
        dynamic_universe = None
        try:
            rurl = os.getenv("REDIS_URL")
            if rurl:
                r = redis.from_url(rurl)
                external = r.smembers("universe:external") or []
                watch = r.smembers("universe:watchlist") or []
                core = [t.strip().upper() for t in (os.getenv("TICKERS", "").split(",")) if t.strip()]
                ext = [x.decode() if isinstance(x, (bytes, bytearray)) else x for x in external]
                wch = [x.decode() if isinstance(x, (bytes, bytearray)) else x for x in watch]
                merged = []
                seen = set()
                max_n = int(os.getenv("UNIVERSE_MAX", "100"))
                for arr in [core, ext, wch]:
                    for s in arr:
                        if s and s not in seen:
                            merged.append(s)
                            seen.add(s)
                        if len(merged) >= max_n:
                            break
                    if len(merged) >= max_n:
                        break
                dynamic_universe = merged
                # Ïù∏Ï†úÏä§ÌÑ∞Ïóê Î∞òÏòÅ Î∞è ÏõåÎ∞çÏóÖ
                try:
                    if hasattr(quotes_ingestor, "update_universe_tickers"):
                        quotes_ingestor.update_universe_tickers(dynamic_universe)
                except Exception:
                    pass
        except Exception:
            dynamic_universe = None

        tickers_iter = dynamic_universe or list(quotes_ingestor.tickers)
        for ticker in tickers_iter:
            try:
                # 1. ÏãúÏÑ∏ Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
                candles = quotes_ingestor.get_latest_candles(ticker, 50)
                if len(candles) < 20:
                    continue
                
                # 2. Í∏∞Ïà†Ï†Å ÏßÄÌëú Í≥ÑÏÇ∞
                indicators = quotes_ingestor.get_technical_indicators(ticker)
                if not indicators:
                    continue
                
                # 2.5 Ïä§Ï∫òÌîÑ Î™®Îìú: ÎßàÏßÄÎßâ Ìïú Ìã±Ïù¥ ÌÅ¨Í≤å ÌäÄÎ©¥ Ï¶âÏãú Ïã†Ìò∏ ÏÉùÏÑ± (ÌÖåÏä§Ìä∏/ÏïåÎ¶ºÏö©)
                try:
                    scalp_enabled = (os.getenv("SCALP_TICK_SPIKE", "false").lower() in ("1", "true", "yes", "on"))
                    if scalp_enabled and len(candles) >= 2:
                        # 30s/15s Î∂ÑÌï† Ïãú ÎßàÏßÄÎßâ Îëê Î∞îÍ∞Ä Í∞ôÏùÄ 1Î∂ÑÏóê ÏÜçÌï® ‚Üí Î∂ÑÎãπ Í∞ÑÍ≤©ÏúºÎ°ú ÎπÑÍµê
                        try:
                            bars_per_min = max(1, 60 // max(getattr(quotes_ingestor, "bar_sec", 60), 1))
                        except Exception:
                            bars_per_min = 1
                        last = candles[-1]
                        prev_idx = -1 - bars_per_min if len(candles) > bars_per_min else -2
                        prev = candles[prev_idx]
                        last_abs_ret = abs((last.c - prev.c) / max(prev.c, 1e-9))
                        spike_thr = float(os.getenv("SCALP_MIN_RET", "0.003"))  # Í∏∞Î≥∏ 0.3%
                        # ÏïºÌõÑ 1Î∂ÑÎ¥â Î∂ÑÌï† Ïãú ÎßàÏßÄÎßâ Îëê Î∞îÍ∞Ä ÎèôÏùº Í∞íÏù∏ Í≤ΩÏö∞Í∞Ä ÎßéÏïÑ Í≥†Ï†ÄÌè≠ Í∏∞Ï§ÄÎèÑ ÌóàÏö©
                        last_range = (last.h - last.l) / max(last.c, 1e-9)
                        range_thr = float(os.getenv("SCALP_MIN_RANGE", "0.003"))  # Í∏∞Î≥∏ 0.3%
                        trig_reason = None
                        if last_abs_ret >= spike_thr:
                            trig_reason = f"ret>={spike_thr:.3%}"
                        elif last_range >= range_thr:
                            trig_reason = f"range>={range_thr:.3%}"
                        if trig_reason:
                            from app.engine.regime import RegimeType, RegimeResult
                            from app.engine.techscore import TechScoreResult
                            direction = 1.0 if (last.c - prev.c) >= 0 else -1.0
                            fake_regime = RegimeResult(
                                regime=RegimeType.VOL_SPIKE,
                                confidence=min(1.0, last_abs_ret * 20.0),
                                features={"last_abs_ret": last_abs_ret},
                                timestamp=datetime.now()
                            )
                            fake_tech = TechScoreResult(
                                score=direction,  # -1 ~ +1Î°ú ÏßÅÏ†ë Ïä§ÏºÄÏùº
                                components={"ema": 0.8 if direction > 0 else 0.2,
                                            "macd": 0.8 if direction > 0 else 0.2,
                                            "rsi": 0.8 if direction > 0 else 0.2,
                                            "vwap": 0.8 if direction > 0 else 0.2},
                                timestamp=datetime.now()
                            )
                            quick_signal = signal_mixer.mix_signals(
                                ticker=ticker,
                                regime_result=fake_regime,
                                tech_score=fake_tech,
                                llm_insight=None,
                                edgar_filing=None,
                                current_price=last.c
                            )
                            if quick_signal:
                                quick_signal.trigger = "tick_spike"
                                quick_signal.summary = f"{trig_reason}; abs_ret={last_abs_ret:.3%}; range={last_range:.3%}"
                                # Ïª∑/ÏÑ∏ÏÖò ÏñµÏ†ú ÎèôÏùº Ï†ÅÏö© (Î°úÏª¨ Í≥ÑÏÇ∞)
                                sess_now = _session_label()
                                cutoff_rth, cutoff_ext = get_signal_cutoffs()
                                cut_r = cutoff_rth
                                cut_e = cutoff_ext
                                cut = cut_r if sess_now == "RTH" else cut_e
                                if abs(quick_signal.score) < cut:
                                    logger.info(f"üî• [SCALP DEBUG] Ïä§Ï∫òÌîÑ Ïã†Ìò∏ ÏñµÏ†ú: {ticker} score={quick_signal.score:.3f} < cut={cut:.3f}")
                                    _record_recent_signal(redis_url=rurl, signal=quick_signal, session_label=sess_now, indicators=indicators, suppressed="below_cutoff")
                                else:
                                    logger.info(f"üî• [SCALP DEBUG] Ïä§Ï∫òÌîÑ Ïã†Ìò∏ Î∞úÌñâ: {ticker} {quick_signal.signal_type.value} score={quick_signal.score:.3f}")
                                    try:
                                        redis_streams.publish_signal({
                                            "ticker": quick_signal.ticker,
                                            "signal_type": quick_signal.signal_type.value,
                                            "score": quick_signal.score,
                                            "confidence": quick_signal.confidence,
                                            "regime": quick_signal.regime,
                                            "tech_score": quick_signal.tech_score,
                                            "sentiment_score": quick_signal.sentiment_score,
                                            "edgar_bonus": quick_signal.edgar_bonus,
                                            "trigger": quick_signal.trigger,
                                            "summary": quick_signal.summary,
                                            "entry_price": quick_signal.entry_price,
                                            "stop_loss": quick_signal.stop_loss,
                                            "take_profit": quick_signal.take_profit,
                                            "horizon_minutes": quick_signal.horizon_minutes,
                                            "timestamp": quick_signal.timestamp.isoformat()
                                        })
                                        logger.info(f"üî• [SCALP DEBUG] Redis Ïä§Ìä∏Î¶º Î∞úÌñâ ÏÑ±Í≥µ: {ticker}")
                                        signals_generated += 1
                                        _record_recent_signal(redis_url=rurl, signal=quick_signal, session_label=sess_now, indicators=indicators)
                                        logger.info(f"Ïä§Ï∫òÌîÑ Ïã†Ìò∏: {ticker} {quick_signal.signal_type.value} ({trig_reason}, abs_ret {last_abs_ret:.2%}, range {last_range:.2%})")
                                    except Exception as e:
                                        logger.error(f"üî• [SCALP DEBUG] Ïä§Ï∫òÌîÑ Redis Î∞úÌñâ Ïã§Ìå®: {ticker} - {e}")
                                    # Ïä§Ï∫òÌîÑ Î™®ÎìúÏóêÏÑ† Ìïú Ìã±Îßå Ïû°ÏúºÎ©¥ Ï∂©Î∂Ñ ‚Äî Îã§Ïùå Ï¢ÖÎ™©ÏúºÎ°ú
                                    continue
                except Exception:
                    pass

                # 2.6 Ïä§Ï∫òÌîÑ Î™®Îìú(ÎåÄÏïà): 3Î∂ÑÎ¥â 3Í∞ú Ïó∞ÏÜç ÏñëÎ¥âÏù¥Î©¥ Î°± Ïã†Ìò∏
                try:
                    if os.getenv("SCALP_3MIN3UP", "false").lower() in ("1", "true", "yes", "on"):
                        bar_sec = int(os.getenv("BAR_SEC", "30") or 30)
                        window = max(1, int(180 / max(bar_sec, 1)))  # 3Î∂Ñ Ï∞ΩÏùò Î∞î Í∞úÏàò
                        need = window * 3
                        if len(candles) >= need:
                            w1 = candles[-window:]
                            w2 = candles[-2*window:-window]
                            w3 = candles[-3*window:-2*window]
                            def _is_green(ws):
                                try:
                                    return (ws[-1].c if ws else 0.0) > (ws[0].o if ws else 0.0)
                                except Exception:
                                    return False
                            if _is_green(w1) and _is_green(w2) and _is_green(w3):
                                from app.engine.regime import RegimeType, RegimeResult
                                from app.engine.techscore import TechScoreResult
                                fake_regime = RegimeResult(
                                    regime=RegimeType.TREND,
                                    confidence=0.8,
                                    features={"3min3up": True},
                                    timestamp=datetime.now()
                                )
                                fake_tech = TechScoreResult(
                                    score=1.0,
                                    components={"ema": 0.9, "macd": 0.9, "rsi": 0.8, "vwap": 0.8},
                                    timestamp=datetime.now()
                                )
                                last_px = candles[-1].c
                                quick_signal = signal_mixer.mix_signals(
                                    ticker=ticker,
                                    regime_result=fake_regime,
                                    tech_score=fake_tech,
                                    llm_insight=None,
                                    edgar_filing=None,
                                    current_price=last_px
                                )
                                if quick_signal:
                                    quick_signal.trigger = "3min_3up"
                                    quick_signal.summary = "3min green x3"
                                    sess_now = _session_label()
                                    cutoff_rth, cutoff_ext = get_signal_cutoffs()
                                    cut_r = cutoff_rth
                                    cut_e = cutoff_ext
                                    cut = cut_r if sess_now == "RTH" else cut_e
                                    if abs(quick_signal.score) < cut:
                                        logger.info(f"üî• [3MIN DEBUG] 3Î∂Ñ3ÏÉÅÏäπ Ïã†Ìò∏ ÏñµÏ†ú: {ticker} score={quick_signal.score:.3f} < cut={cut:.3f}")
                                        _record_recent_signal(redis_url=rurl, signal=quick_signal, session_label=sess_now, indicators=indicators, suppressed="below_cutoff")
                                    else:
                                        logger.info(f"üî• [3MIN DEBUG] 3Î∂Ñ3ÏÉÅÏäπ Ïã†Ìò∏ Î∞úÌñâ: {ticker} long score={quick_signal.score:.3f}")
                                        try:
                                            redis_streams.publish_signal({
                                                "ticker": quick_signal.ticker,
                                                "signal_type": quick_signal.signal_type.value,
                                                "score": quick_signal.score,
                                                "confidence": quick_signal.confidence,
                                                "regime": quick_signal.regime,
                                                "tech_score": quick_signal.tech_score,
                                                "sentiment_score": quick_signal.sentiment_score,
                                                "edgar_bonus": quick_signal.edgar_bonus,
                                                "trigger": quick_signal.trigger,
                                                "summary": quick_signal.summary,
                                                "entry_price": quick_signal.entry_price,
                                                "stop_loss": quick_signal.stop_loss,
                                                "take_profit": quick_signal.take_profit,
                                                "horizon_minutes": quick_signal.horizon_minutes,
                                                "timestamp": quick_signal.timestamp.isoformat()
                                            })
                                            logger.info(f"üî• [3MIN DEBUG] Redis Ïä§Ìä∏Î¶º Î∞úÌñâ ÏÑ±Í≥µ: {ticker}")
                                            signals_generated += 1
                                            _record_recent_signal(redis_url=rurl, signal=quick_signal, session_label=sess_now, indicators=indicators)
                                            logger.info(f"Ïä§Ï∫òÌîÑ Ïã†Ìò∏: {ticker} long (3min_3up)")
                                        except Exception as e:
                                            logger.error(f"üî• [3MIN DEBUG] 3Î∂Ñ3ÏÉÅÏäπ Redis Î∞úÌñâ Ïã§Ìå®: {ticker} - {e}")
                                        continue
                except Exception:
                    pass

                # 3. Î†àÏßê Í∞êÏßÄ
                regime_result = regime_detector.detect_regime(candles)
                
                # 4. Í∏∞Ïà†Ï†Å Ï†êÏàò Í≥ÑÏÇ∞
                tech_score = tech_score_engine.calculate_tech_score(candles)
                
                # 5. EDGAR Í≥µÏãú ÌôïÏù∏
                edgar_filing = None
                llm_insight = None
                
                # ÏµúÍ∑º EDGAR Í≥µÏãúÍ∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
                recent_edgar = get_recent_edgar_filing(ticker)
                if recent_edgar:
                    edgar_filing = recent_edgar
                    # LLM Î∂ÑÏÑù (EDGAR Ïù¥Î≤§Ìä∏Ïù¥ÎØÄÎ°ú Ï°∞Í±¥ Ï∂©Ï°±)
                    llm_insight = llm_engine.analyze_edgar_filing(edgar_filing)
                
                # 6. Î†àÏßêÏù¥ vol_spikeÏù∏ Í≤ΩÏö∞ Ï∂îÍ∞Ä LLM Î∂ÑÏÑù
                if regime_result.regime.value == 'vol_spike' and llm_engine and not llm_insight:
                    # vol_spike Î†àÏßêÏóêÏÑú LLM Î∂ÑÏÑù (Ï°∞Í±¥ Ï∂©Ï°±)
                    text = f"Volatility spike detected for {ticker} in {regime_result.regime.value} regime"
                    llm_insight = llm_engine.analyze_text(text, f"vol_spike_{ticker}", regime='vol_spike')
                
                # 6. Ïû•Ïô∏ Ï†ÑÏö© pre-filter (ÏÑ∏ÏÖò/Ïú†ÎèôÏÑ±/Ïä§ÌîÑÎ†àÎìú/Ïø®Îã§Ïö¥/ÏùºÏùºÏÉÅÌïú)
                ext_enabled = (os.getenv("EXTENDED_PRICE_SIGNALS", "false").lower() in ("1","true","yes","on"))
                session_label = _session_label()
                cutoff_rth, cutoff_ext = get_signal_cutoffs()
                dvol5m = float(indicators.get("dollar_vol_5m", 0.0))
                spread_bp = float(indicators.get("spread_bp", 0.0))
                suppress_reason = None
                if session_label == "EXT":
                    if not ext_enabled:
                        suppress_reason = "ext_disabled"
                    if dvol5m < float(os.getenv("EXT_MIN_DOLLAR_VOL_5M", "50000")):
                        suppress_reason = suppress_reason or "low_dvol"
                    if spread_bp > float(os.getenv("EXT_MAX_SPREAD_BP", "300")):
                        suppress_reason = suppress_reason or "wide_spread"
                    # Ïø®Îã§Ïö¥/ÏùºÏùº ÏÉÅÌïú Ï≤¥ÌÅ¨: Redis ÌÇ§ ÏÇ¨Ïö©
                    try:
                        if rurl:
                            r = redis.from_url(rurl)
                            cool_min = int(os.getenv("EXT_COOLDOWN_MIN", "7"))
                            daily_cap = 3
                            now_ts = int(time.time())
                            cd_key = f"cooldown:{ticker}"
                            last_ts = int(r.get(cd_key) or 0)
                            if now_ts - last_ts < cool_min * 60:
                                suppress_reason = suppress_reason or "cooldown"
                            day_key = f"dailycap:{datetime.utcnow():%Y%m%d}:{ticker}"
                            used = int(r.get(day_key) or 0)
                            if used >= daily_cap:
                                suppress_reason = suppress_reason or "daily_cap"
                            if not suppress_reason:
                                # ÌÜµÍ≥º Ïãú Îã§Ïùå Ïπ¥Ïö¥ÌåÖÏùÑ ÏúÑÌï¥ ÏûÑÏãú ÎßàÌÇπ(Ïã§Ï†ú Slack Ï†ÑÏÜ° Ïãú ÏµúÏ¢Ö Ï¶ùÍ∞Ä Í∂åÏû•)
                                r.setex(cd_key, cool_min*60, now_ts)
                    except Exception:
                        pass

                # VaR95 Í≤ΩÎüâ Í∞ÄÎìú: ÏµúÍ∑º Î¶¨ÌÑ¥ ÏÉòÌîå Í∏∞Î∞ò(ÏòµÏÖò)
                try:
                    from app.engine.risk import rolling_var95
                    # ÏÉòÌîåÏùÄ Î≥ÑÎèÑ Í≥≥ÏóêÏÑú Ï±ÑÏõåÏßÑÎã§Í≥† Í∞ÄÏ†ï; ÏóÜÏúºÎ©¥ Ïä§ÌÇµ
                    rurl = os.getenv("REDIS_URL")
                    var_guard = False
                    if rurl:
                        r = redis.from_url(rurl)
                        key = f"risk:rets:{ticker}:{regime_result.regime.value}"
                        samples = [float(x) for x in (r.lrange(key, 0, 9999) or [])]
                        if samples:
                            var95 = rolling_var95(samples)
                            # Í∞ÑÏù¥ Í∏∞Ï§Ä: ÏòàÏÉÅ ÏÜêÏã§R>VaRÏù¥Î©¥ ÏñµÏ†ú
                            # Ïó¨Í∏∞ÏÑúÎäî Ïã†Ìò∏ ÏÉùÏÑ± ÌõÑ Ïª∑Ïò§ÌîÑ Îã®Í≥ÑÏóêÏÑú suppress
                            pass
                except Exception:
                    pass

                # 7. ÏãúÍ∑∏ÎÑê ÎØπÏã±
                current_price = candles[-1].c if candles else 0
                signal = signal_mixer.mix_signals(
                    ticker=ticker,
                    regime_result=regime_result,
                    tech_score=tech_score,
                    llm_insight=llm_insight,
                    edgar_filing=edgar_filing,
                    current_price=current_price
                )
                
                if signal:
                    # Ïª∑Ïò§ÌîÑ Ï†ÅÏö© (ÏÑ∏ÏÖòÎ≥Ñ)
                    cut = cutoff_rth if session_label == "RTH" else cutoff_ext
                    if abs(signal.score) < cut or suppress_reason:
                        # ÏñµÏ†ú ÏÇ¨Ïú† ÏÉÅÏÑ∏ Î°úÍ∑∏
                        logger.info(f"ÏñµÏ†ú: reason={suppress_reason or 'below_cutoff'} "
                                   f"score={signal.score:.3f} cut={cut:.3f} dvol5m={dvol5m:.0f} spread_bp={spread_bp:.1f}")
                        
                        # ÏñµÏ†ú Î©îÌä∏Î¶≠ ÎàÑÏ†Å
                        try:
                            if rurl:
                                r = redis.from_url(rurl)
                                hkey = f"metrics:suppressed:{datetime.utcnow():%Y%m%d}"
                                r.hincrby(hkey, suppress_reason or "below_cutoff", 1)
                        except Exception:
                            pass
                        # ÏµúÍ∑º Ïã†Ìò∏ Î¶¨Ïä§Ìä∏Ïóê suppressedÎ°ú Í∏∞Î°ù
                        _record_recent_signal(redis_url=rurl, signal=signal, session_label=session_label, indicators=indicators, suppressed=suppress_reason or "below_cutoff")
                        continue

                    # 7. Redis Ïä§Ìä∏Î¶ºÏóê Î∞úÌñâ
                    signal_data = {
                        "ticker": signal.ticker,
                        "signal_type": signal.signal_type.value,
                        "score": signal.score,
                        "confidence": signal.confidence,
                        "regime": signal.regime,
                        "tech_score": signal.tech_score,
                        "sentiment_score": signal.sentiment_score,
                        "edgar_bonus": signal.edgar_bonus,
                        "trigger": signal.trigger,
                        "summary": signal.summary,
                        "entry_price": signal.entry_price,
                        "stop_loss": signal.stop_loss,
                        "take_profit": signal.take_profit,
                        "horizon_minutes": signal.horizon_minutes,
                        "timestamp": signal.timestamp.isoformat()
                    }
                    
                    logger.info(f"üî• [DEBUG] ÏãúÍ∑∏ÎÑê ÏÉùÏÑ±Îê®! ticker={ticker}, type={signal.signal_type.value}, score={signal.score:.3f}, cut={cut:.3f}")
                    logger.info(f"üî• [DEBUG] Redis Ïä§Ìä∏Î¶º Î∞úÌñâ ÏãúÎèÑ Ï§ë...")
                    
                    try:
                        redis_streams.publish_signal(signal_data)
                        logger.info(f"üî• [DEBUG] Redis Ïä§Ìä∏Î¶º Î∞úÌñâ ÏÑ±Í≥µ: {ticker}")
                        
                        # Slack Ï†ÑÏÜ°: Í∞ïÏã†Ìò∏Îßå (ÏõêÎûò Í∏∞Ìöç - ÏÜåÏàò¬∑ÍµµÏßÅÌïú ÏïåÎ¶º)
                        if slack_bot:
                            # Í∞ïÏã†Ìò∏ Í∏∞Ï§Ä: abs(score) >= cut + 0.10
                            strong_signal_threshold = cut + 0.10
                            is_strong_signal = abs(signal.score) >= strong_signal_threshold
                            
                            if is_strong_signal:
                                logger.info(f"üì¢ Slack Ï†ÑÏÜ° (Í∞ïÏã†Ìò∏): {ticker} score={signal.score:.3f} >= {strong_signal_threshold:.3f}")
                                try:
                                    slack_message = format_slack_message(signal)
                                    result = slack_bot.send_message(slack_message)
                                    if result:
                                        logger.info(f"‚úÖ Slack Ï†ÑÏÜ° ÏÑ±Í≥µ: {ticker}")
                                    else:
                                        logger.error(f"‚ùå Slack Ï†ÑÏÜ° Ïã§Ìå®: {ticker}")
                                except Exception as e:
                                    logger.error(f"‚ùå Slack Ï†ÑÏÜ° ÏòàÏô∏: {ticker} - {e}")
                            else:
                                logger.info(f"üîá Slack Ï†ÑÏÜ° ÏñµÏ†ú (ÏïΩÏã†Ìò∏): {ticker} score={signal.score:.3f} < {strong_signal_threshold:.3f}")
                        else:
                            logger.warning(f"üîç Slack Ï†ÑÏÜ° Í±¥ÎÑàÎúÄ - slack_botÏù¥ None: {ticker}")
                        
                        signals_generated += 1
                    except Exception as e:
                        logger.error(f"üî• [DEBUG] Redis Ïä§Ìä∏Î¶º Î∞úÌñâ Ïã§Ìå®: {ticker} - {e}")
                        continue
                    # ÏµúÍ∑º Ïã†Ìò∏ Í∏∞Î°ù (+ ÏÑ∏ÏÖò/Ïä§ÌîÑÎ†àÎìú/Îã¨Îü¨ÎåÄÍ∏à)
                    _record_recent_signal(redis_url=rurl, signal=signal, session_label=session_label, indicators=indicators)
                    
                    logger.info(f"ÏãúÍ∑∏ÎÑê ÏÉùÏÑ±: {ticker} {signal.signal_type.value} (Ï†êÏàò: {signal.score:.2f})")
                
            except Exception as e:
                logger.error(f"ÏãúÍ∑∏ÎÑê ÏÉùÏÑ± Ïã§Ìå® ({ticker}): {e}")
                continue
        
        execution_time = time.time() - start_time
        logger.info(f"ÏãúÍ∑∏ÎÑê ÏÉùÏÑ± ÏôÑÎ£å: {signals_generated}Í∞ú, {execution_time:.2f}Ï¥à")
        
        return {
            "status": "success",
            "signals_generated": signals_generated,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ÏãúÍ∑∏ÎÑê ÏÉùÏÑ± ÏûëÏóÖ Ïã§Ìå®: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@celery_app.task(bind=True, name="app.jobs.scheduler.update_quotes")
def update_quotes(self):
    """ÏãúÏÑ∏ ÏóÖÎç∞Ïù¥Ìä∏ ÏûëÏóÖ"""
    try:
        start_time = time.time()
        logger.debug("ÏãúÏÑ∏ ÏóÖÎç∞Ïù¥Ìä∏ ÏãúÏûë")
        
        quotes_ingestor = trading_components["quotes_ingestor"]
        redis_streams = trading_components["redis_streams"]
        
        if not quotes_ingestor or not redis_streams:
            return {"status": "skipped", "reason": "components_not_ready"}
        
        # Î™®Îì† Ï¢ÖÎ™© ÏãúÏÑ∏ ÏóÖÎç∞Ïù¥Ìä∏
        quotes_ingestor.update_all_tickers()
        
        # Redis Ïä§Ìä∏Î¶ºÏóê Î∞úÌñâ
        market_data = quotes_ingestor.get_market_data_summary()
        # ÏòµÏÖò: ÏïºÌõÑ Ïù∏Ï†úÏä§ÌÑ∞ ÏöîÏïΩ Î°úÍ∑∏ (Ïã§ÏãúÍ∞Ñ Ìã± Ï∂îÏ†ÅÏö©)
        try:
            if os.getenv("QUOTE_LOG_VERBOSE", "false").lower() in ("1", "true", "yes", "on"):
                for _t, _d in (market_data or {}).items():
                    _ind = _d.get("indicators", {}) if isinstance(_d, dict) else {}
                    _px = float(_d.get("current_price", 0.0) or 0.0)
                    _dv = float(_ind.get("dollar_vol_5m", 0.0) or 0.0)
                    _sp = float(_ind.get("spread_bp", 0.0) or 0.0)
                    _ts = _d.get("last_update")
                    logger.info(f"[YQ] { _t } px={_px:.4f} dollar_vol_5m={_dv:.0f} spread_bp={_sp:.1f} ts={_ts}")
        except Exception:
            pass
        for ticker, data in market_data.items():
            if data.get("current_price"):
                quote_data = {
                    "ticker": ticker,
                    "price": data["current_price"],
                    "indicators": data.get("indicators", {}),
                    "timestamp": data.get("last_update", datetime.now()).isoformat()
                }
                redis_streams.publish_quote(ticker, "XNAS", quote_data)
        
        execution_time = time.time() - start_time
        logger.debug(f"ÏãúÏÑ∏ ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {len(market_data)}Í∞ú Ï¢ÖÎ™©, {execution_time:.2f}Ï¥à")
        
        return {
            "status": "success",
            "tickers_updated": len(market_data),
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ÏãúÏÑ∏ ÏóÖÎç∞Ïù¥Ìä∏ ÏûëÏóÖ Ïã§Ìå®: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@celery_app.task(bind=True, name="app.jobs.scheduler.scan_edgar")
def scan_edgar(self):
    """EDGAR Ïä§Ï∫î ÏûëÏóÖ"""
    try:
        start_time = time.time()
        logger.debug("EDGAR Ïä§Ï∫î ÏãúÏûë")
        
        # EDGAR Ïä§Ï∫êÎÑà Ï§ÄÎπÑ
        try:
            from app.io.edgar import EDGARScanner
        except Exception:
            EDGARScanner = None  # type: ignore
        edgar_scanner = trading_components.get("edgar_scanner")
        if edgar_scanner is None and EDGARScanner is not None:
            edgar_scanner = EDGARScanner()
            trading_components["edgar_scanner"] = edgar_scanner
        redis_streams = trading_components.get("redis_streams")
        redis_client = None
        try:
            # dedupeÎ•º ÏúÑÌïú raw redis ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï†ëÍ∑º (streams ÎÇ¥Î∂Ä ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïû¨ÏÇ¨Ïö©)
            if redis_streams:
                redis_client = redis_streams.redis_client
        except Exception:
            redis_client = None
        
        if not redis_streams:
            return {"status": "skipped", "reason": "components_not_ready"}
        
        # EDGAR Í≥µÏãú Ïä§Ï∫î
        filings = edgar_scanner.run_scan()
        
        # Redis Ïä§Ìä∏Î¶ºÏóê Î∞úÌñâ (Ï§ëÎ≥µ Î∞©ÏßÄ)
        dedupe_key = "edgar:dedupe:snippets"
        published = 0
        for filing in filings:
            snippet_text = filing.get("summary") or filing.get("snippet_text") or ""
            url = filing.get("url", "")
            base = (snippet_text or url).encode()
            snippet_hash = hashlib.md5(base).hexdigest()
            if redis_client:
                # Ï§ëÎ≥µÏù¥Î©¥ Ïä§ÌÇµ
                added = redis_client.sadd(dedupe_key, snippet_hash)
                if not added:
                    continue
            # Ìï¥ÏãúÎ•º Ìï®Íªò Ï†ÄÏû•Ìï¥ÎëêÍ∏∞
            filing["snippet_hash"] = snippet_hash
            # StreamsÎäî Î¨∏ÏûêÏó¥ Í∞íÏù¥ ÏïàÏ†ÑÌïòÎØÄÎ°ú dict/listÎäî JSON Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
            payload = {k: json.dumps(v) if isinstance(v, (dict, list)) else (v if v is not None else "") for k, v in filing.items()}
            redis_streams.publish_edgar(payload)
            published += 1
            
            # Ï§ëÏöî Í≥µÏãú LLM Ï≤òÎ¶¨
            if filing.get("impact_score", 0) > 0.7 and llm_engine:
                llm_insight = llm_engine.analyze_edgar_filing(filing)
                if llm_insight:
                    insight_data = {
                        "ticker": filing["ticker"],
                        "sentiment": llm_insight.sentiment,
                        "trigger": llm_insight.trigger,
                        "horizon_minutes": llm_insight.horizon_minutes,
                        "summary": llm_insight.summary,
                        "timestamp": llm_insight.timestamp.isoformat()
                    }
                    redis_streams.publish_news(insight_data)
        
        execution_time = time.time() - start_time
        logger.debug(f"EDGAR Ïä§Ï∫î ÏôÑÎ£å: {published}Í∞ú Î∞úÌñâ, {execution_time:.2f}Ï¥à")
        
        return {
            "status": "success",
            "published": published,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"EDGAR Ïä§Ï∫î ÏûëÏóÖ Ïã§Ìå®: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@celery_app.task(bind=True, name="app.jobs.scheduler.check_risk")
def check_risk(self):
    """Î¶¨Ïä§ÌÅ¨ Ï≤¥ÌÅ¨ ÏûëÏóÖ"""
    try:
        start_time = time.time()
        logger.debug("Î¶¨Ïä§ÌÅ¨ Ï≤¥ÌÅ¨ ÏãúÏûë")
        
        risk_engine = trading_components["risk_engine"]
        slack_bot = trading_components["slack_bot"]
        redis_streams = trading_components["redis_streams"]
        
        if not risk_engine:
            return {"status": "skipped", "reason": "components_not_ready"}
        
        # Î¶¨Ïä§ÌÅ¨ ÏßÄÌëú Í≥ÑÏÇ∞
        risk_metrics = risk_engine.calculate_risk_metrics()
        
        # Redis Ïä§Ìä∏Î¶ºÏóê Î∞úÌñâ
        if redis_streams:
            risk_data = {
                "daily_pnl": risk_metrics.daily_pnl,
                "daily_pnl_pct": risk_metrics.daily_pnl_pct,
                "var_95": risk_metrics.var_95,
                "max_drawdown": risk_metrics.max_drawdown,
                "position_count": risk_metrics.position_count,
                "total_exposure": risk_metrics.total_exposure,
                "status": risk_metrics.status.value,
                "timestamp": risk_metrics.timestamp.isoformat()
            }
            redis_streams.publish_risk_update(risk_data)
        
        # Í≤ΩÍ≥†/ÏúÑÌóò ÏÉÅÌÉúÏùº Îïå Slack ÏïåÎ¶º
        if risk_metrics.status.value in ["warning", "critical", "shutdown"] and slack_bot:
            risk_report = risk_engine.get_risk_report()
            slack_bot.send_risk_alert(risk_report)
        
        execution_time = time.time() - start_time
        logger.debug(f"Î¶¨Ïä§ÌÅ¨ Ï≤¥ÌÅ¨ ÏôÑÎ£å: {risk_metrics.status.value}, {execution_time:.2f}Ï¥à")
        
        return {
            "status": "success",
            "risk_status": risk_metrics.status.value,
            "daily_pnl_pct": risk_metrics.daily_pnl_pct,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Î¶¨Ïä§ÌÅ¨ Ï≤¥ÌÅ¨ ÏûëÏóÖ Ïã§Ìå®: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@celery_app.task(bind=True, name="app.jobs.scheduler.daily_reset")
def daily_reset(self):
    """ÏùºÏùº Î¶¨ÏÖã ÏûëÏóÖ"""
    try:
        logger.info("ÏùºÏùº Î¶¨ÏÖã ÏãúÏûë")
        
        risk_engine = trading_components["risk_engine"]
        paper_ledger = trading_components["paper_ledger"]
        slack_bot = trading_components["slack_bot"]
        
        # Î¶¨Ïä§ÌÅ¨ ÏóîÏßÑ Î¶¨ÏÖã
        if risk_engine:
            risk_engine.reset_daily()
        
        # ÌéòÏù¥Ìçº Î†àÏ†Ä Î¶¨ÏÖã
        if paper_ledger:
            paper_ledger.reset_daily()
        
        # Slack ÏïåÎ¶º
        if slack_bot:
            message = {
                "text": "üîÑ ÏùºÏùº Î¶¨ÏÖãÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§",
                "channel": "#trading-signals"
            }
            slack_bot.send_message(message)
        
        logger.info("ÏùºÏùº Î¶¨ÏÖã ÏôÑÎ£å")
        
        return {
            "status": "success",
            "message": "ÏùºÏùº Î¶¨ÏÖã ÏôÑÎ£å",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ÏùºÏùº Î¶¨ÏÖã ÏûëÏóÖ Ïã§Ìå®: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@celery_app.task(name="app.jobs.scheduler.daily_report")
def daily_report(force=False, post=True):
    """ÏùºÏùº Î¶¨Ìè¨Ìä∏ ÏûëÏóÖ"""
    try:
        logger.info("ÏùºÏùº Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± ÏãúÏûë")
        
        paper_ledger = trading_components["paper_ledger"]
        risk_engine = trading_components["risk_engine"]
        llm_engine = trading_components["llm_engine"]
        slack_bot = trading_components["slack_bot"]
        
        if not paper_ledger:
            return {"status": "skipped", "reason": "components_not_ready"}
        
        # ÏùºÏùº ÌÜµÍ≥Ñ ÏàòÏßë
        daily_stats = paper_ledger.get_daily_stats()
        
        # Î¶¨Ïä§ÌÅ¨ ÏßÄÌëú
        risk_metrics = None
        if risk_engine:
            risk_metrics = risk_engine.get_risk_report()
        
        # LLM ÏÇ¨Ïö©Îüâ
        llm_usage = None
        if llm_engine:
            llm_usage = llm_engine.get_status()
        
        # Î¶¨Ìè¨Ìä∏ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
        report_data = {
            "trades": daily_stats.get("trades", 0),
            "realized_pnl": daily_stats.get("realized_pnl", 0),
            "win_rate": 0.6,  # Ïã§Ï†úÎ°úÎäî Í≥ÑÏÇ∞ ÌïÑÏöî
            "avg_rr": 1.4,    # Ïã§Ï†úÎ°úÎäî Í≥ÑÏÇ∞ ÌïÑÏöî
            "risk_metrics": risk_metrics,
            "llm_usage": llm_usage
        }
        
        # Slack Î¶¨Ìè¨Ìä∏ Ï†ÑÏÜ°
        if slack_bot and post:
            slack_bot.send_daily_report(report_data)
        
        logger.info("ÏùºÏùº Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± ÏôÑÎ£å")
        
        return {
            "status": "success",
            "trades": report_data["trades"],
            "realized_pnl": report_data["realized_pnl"],
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ÏùºÏùº Î¶¨Ìè¨Ìä∏ ÏûëÏóÖ Ïã§Ìå®: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

def get_recent_edgar_filing(ticker: str) -> Optional[Dict]:
    """ÏµúÍ∑º EDGAR Í≥µÏãú Ï°∞Ìöå (Ï∫êÏãúÎêú Îç∞Ïù¥ÌÑ∞ÏóêÏÑú)"""
    # Ïã§Ï†úÎ°úÎäî Ï∫êÏãúÎÇò Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ï°∞Ìöå
    # Ïó¨Í∏∞ÏÑúÎäî Í∞ÑÎã®Ìûà None Î∞òÌôò
    return None

def _session_label() -> str:
    """RTH/EXT Í∞ÑÎã® ÌåêÎ≥Ñ: ET ÏãúÍ∞ÑÏúºÎ°ú 09:30-16:00ÏùÄ RTH, Í∑∏ Ïô∏ 04:00-20:00ÏùÄ EXT, ÎÇòÎ®∏ÏßÄÎäî CLOSED"""
    from datetime import datetime, timedelta, time as dtime, timezone
    et_tz = timezone(timedelta(hours=-5))
    now_est = datetime.now(et_tz)
    # DST Í∞ÑÏù¥ Ï†ÅÏö©
    dst_start = now_est.replace(month=3, day=8 + (6 - now_est.replace(month=3, day=1).weekday()) % 7, hour=2)
    dst_end = now_est.replace(month=11, day=1 + (6 - now_est.replace(month=11, day=1).weekday()) % 7, hour=2)
    if dst_start <= now_est < dst_end:
        et_tz = timezone(timedelta(hours=-4))
    now = datetime.now(et_tz).time()
    if dtime(9,30) <= now <= dtime(16,0):
        return "RTH"
    if dtime(4,0) <= now <= dtime(20,0):
        return "EXT"
    return "CLOSED"

def _record_recent_signal(redis_url: Optional[str], signal, session_label: str, indicators: Dict, suppressed: Optional[str] = None) -> None:
    try:
        if not redis_url:
            return
        r = redis.from_url(redis_url)
        key = "signals:recent"
        payload = {
            "ticker": signal.ticker,
            "signal_type": signal.signal_type.value,
            "score": signal.score,
            "confidence": signal.confidence,
            "regime": signal.regime,
            "timestamp": signal.timestamp.isoformat(),
            "session": session_label,
            "spread_bp": float(indicators.get("spread_bp", 0.0)),
            "dollar_vol_5m": float(indicators.get("dollar_vol_5m", 0.0)),
        }
        if suppressed:
            payload["suppressed_reason"] = suppressed
        r.lpush(key, json.dumps(payload))
        r.ltrim(key, 0, 500)
    except Exception:
        pass

def initialize_components(components: Dict):
    """Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî"""
    global trading_components
    trading_components.update(components)
    logger.info("Ïä§ÏºÄÏ§ÑÎü¨ Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")

    # Quotes ingestor Í∏∞Î≥∏ Ï£ºÏûÖ(ÎîúÎ†àÏù¥Îìú)
    try:
        from app.io.quotes_delayed import DelayedQuotesIngestor
        if trading_components.get("quotes_ingestor") is None:
            trading_components["quotes_ingestor"] = DelayedQuotesIngestor()
            logger.info("ÎîúÎ†àÏù¥Îìú Quotes Ïù∏Ï†úÏä§ÌÑ∞ Ï¥àÍ∏∞Ìôî")
    except Exception as e:
        logger.warning(f"Quotes Ïù∏Ï†úÏä§ÌÑ∞ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")


@celery_app.task(bind=True, name="app.jobs.scheduler.ingest_edgar_stream")
def ingest_edgar_stream(self):
    """Redis Streams(news.edgar) ‚Üí DB(edgar_events) Ï†ÅÏû¨. DB UNIQUE(snippet_hash)Î°ú ÏµúÏ¢Ö dedupe.
    ÏõåÏª§ Î∂ÄÌåÖ ÌõÑ readiness ÌÜµÍ≥º Ïãú Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú Ïã§ÌñâÎêúÎã§.
    """
    try:
        from app.io.streams import RedisStreams, StreamConsumer
        rs = trading_components.get("redis_streams")
        if rs is None:
            # envÏóêÏÑú Ïó∞Í≤∞
            import urllib.parse as _u
            redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
            parsed = _u.urlparse(redis_url)
            host = parsed.hostname or "redis"
            port = int(parsed.port or 6379)
            db = int(parsed.path.lstrip("/") or 0)
            rs = RedisStreams(host=host, port=port, db=db)
            trading_components["redis_streams"] = rs
        consumer = trading_components.get("stream_consumer") or StreamConsumer(rs)
        trading_components["stream_consumer"] = consumer

        # DB Ïó∞Í≤∞ ÌôïÎ≥¥/Ïû¨ÏÇ¨Ïö©
        conn = trading_components.get("db_connection")
        if conn is None or conn.closed:
            dsn = os.getenv("POSTGRES_URL") or os.getenv("DATABASE_URL")
            if not dsn:
                return {"status": "skipped", "reason": "missing_db_dsn", "timestamp": datetime.now().isoformat()}
            conn = psycopg2.connect(dsn)
            conn.autocommit = True
            trading_components["db_connection"] = conn

        messages = consumer.consume_edgar_events(count=20, block_ms=100)
        inserted = 0
        with conn.cursor() as cur:
            for msg in messages:
                d = msg.data
                # Î¨∏ÏûêÏó¥Î°ú Ïò® JSONÏùÑ ÏõêÏÉÅÎ≥µÍµ¨ ÏãúÎèÑ
                def _norm(key):
                    val = d.get(key)
                    if val is None:
                        return None
                    try:
                        return json.loads(val)
                    except Exception:
                        return val

                ticker = _norm("ticker") or d.get("ticker")
                form = _norm("form") or d.get("form")
                item = _norm("item") or d.get("item")
                url = _norm("url") or d.get("url")
                snippet_text = _norm("snippet_text") or d.get("snippet_text")
                snippet_hash = _norm("snippet_hash") or d.get("snippet_hash")
                
                # formÏù¥ NoneÏù¥Î©¥ snippet_textÏóêÏÑú Ï∂îÏ∂ú ÏãúÎèÑ
                if form is None and snippet_text:
                    import re
                    match = re.search(r'\b(4|8-K|10-K|10-Q|S-1|S-3|DEF 14A)\b', str(snippet_text))
                    if match:
                        form = match.group(1)

                cur.execute(
                    """
                    INSERT INTO edgar_events (ticker, form, item, url, snippet_hash, snippet_text)
                    VALUES (%s, %s, %s, %s, %s, %s)
                    ON CONFLICT (snippet_hash) DO NOTHING
                    """,
                    (str(ticker) if ticker is not None else None,
                     str(form) if form is not None else None,
                     str(item) if item is not None else None,
                     str(url) if url is not None else None,
                     str(snippet_hash) if snippet_hash is not None else None,
                     str(snippet_text) if snippet_text is not None else None)
                )
                consumer.acknowledge("news.edgar", msg.message_id)
                inserted += 1

        return {"status": "success", "inserted": inserted, "timestamp": datetime.now().isoformat()}
    except Exception as e:
        logger.error(f"EDGAR Ïä§Ìä∏Î¶º Ï†ÅÏû¨ Ïã§Ìå®: {e}")
        return {"status": "error", "error": str(e), "timestamp": datetime.now().isoformat()}

def get_task_status(task_id: str) -> Dict:
    """ÏûëÏóÖ ÏÉÅÌÉú Ï°∞Ìöå"""
    try:
        result = celery_app.AsyncResult(task_id)
        return {
            "task_id": task_id,
            "status": result.status,
            "result": result.result if result.ready() else None,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ÏûëÏóÖ ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
        return {
            "task_id": task_id,
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@celery_app.task(name="app.jobs.scheduler.refresh_universe")
def refresh_universe():
    """15Î∂ÑÎßàÎã§ Ïú†ÎãàÎ≤ÑÏä§ Î≥ëÌï©‚ÜíÏù∏Ï†úÏä§ÌÑ∞ Î∞òÏòÅ"""
    try:
        quotes_ingestor = trading_components.get("quotes_ingestor")
        rurl = os.getenv("REDIS_URL")
        if not quotes_ingestor or not rurl:
            return {"status": "skipped"}
        r = redis.from_url(rurl)
        external = r.smembers("universe:external") or []
        watch = r.smembers("universe:watchlist") or []
        core = [t.strip().upper() for t in (os.getenv("TICKERS", "").split(",")) if t.strip()]
        ext = [x.decode() if isinstance(x, (bytes, bytearray)) else x for x in external]
        wch = [x.decode() if isinstance(x, (bytes, bytearray)) else x for x in watch]
        merged = []
        seen = set()
        max_n = int(os.getenv("UNIVERSE_MAX", "60"))
        for arr in [core, ext, wch]:
            for s in arr:
                if s and s not in seen:
                    merged.append(s)
                    seen.add(s)
                if len(merged) >= max_n:
                    break
            if len(merged) >= max_n:
                break
        if hasattr(quotes_ingestor, "update_universe_tickers"):
            quotes_ingestor.update_universe_tickers(merged)
        return {"status": "ok", "universe_size": len(merged)}
    except Exception as e:
        logger.error(f"Ïú†ÎãàÎ≤ÑÏä§ Í∞±Ïã† Ïã§Ìå®: {e}")
        return {"status": "error", "error": str(e)}

@celery_app.task(name="app.jobs.scheduler.adaptive_cutoff")
def adaptive_cutoff():
    """Ï†ÑÏùº Ï≤¥Í≤∞ Í±¥Ïàò Í∏∞Î∞ò Ïª∑Ïò§ÌîÑ Ï°∞Ï†ï: cfg:signal_cutoff:{rth|ext} ¬±0.02 with bounds"""
    try:
        if os.getenv("ADAPTIVE_CUTOFF_ENABLED", "true").lower() not in ("1","true","yes","on"):
            return {"status": "skipped", "reason": "disabled"}
        rurl = os.getenv("REDIS_URL")
        if not rurl:
            return {"status": "skipped", "reason": "no_redis"}
        r = redis.from_url(rurl)
        # Ï≤¥Í≤∞ Í±¥Ïàò: Í∞ÑÏù¥ ÏßëÍ≥Ñ(orders_paper ÌÖåÏù¥Î∏î ÏóÜÏúºÎ©¥ 0 Ï≤òÎ¶¨)
        fills = 0
        try:
            dsn = os.getenv("POSTGRES_URL") or os.getenv("DATABASE_URL")
            if dsn:
                import psycopg2
                from datetime import date, timedelta
                conn = psycopg2.connect(dsn)
                cur = conn.cursor()
                prev = (datetime.utcnow() - timedelta(days=1)).date()
                cur.execute("SELECT COUNT(*) FROM orders_paper WHERE DATE(ts)=%s", (prev,))
                fills = int(cur.fetchone()[0] or 0)
                cur.close(); conn.close()
        except Exception:
            fills = 0
        # ÌòÑÏû¨Í∞í ÏùΩÍ∏∞
        def _getf(k, d):
            v = r.get(k)
            try:
                return float(v) if v is not None else d
            except Exception:
                return d
        rth = _getf("cfg:signal_cutoff:rth", settings.SIGNAL_CUTOFF_RTH)
        ext = _getf("cfg:signal_cutoff:ext", settings.SIGNAL_CUTOFF_EXT)
        # Ï°∞Ï†ï (ÌòÑÏã§Ï†Å Í≤ΩÍ≥ÑÍ∞íÏúºÎ°ú ÏàòÏ†ï)
        rth_base, ext_base = settings.SIGNAL_CUTOFF_RTH, settings.SIGNAL_CUTOFF_EXT
        delta = -0.02 if fills == 0 else (0.02 if fills >= 4 else 0.0)
        rth_new = min(max(rth + delta, max(0.12, rth_base - 0.06)), rth_base + 0.12)
        ext_new = min(max(ext + delta, max(0.18, ext_base - 0.10)), ext_base + 0.10)
        r.set("cfg:signal_cutoff:rth", rth_new)
        r.set("cfg:signal_cutoff:ext", ext_new)
        return {"status": "ok", "fills": fills, "rth": rth_new, "ext": ext_new}
    except Exception as e:
        logger.error(f"Ï†ÅÏùëÌòï Ïª∑Ïò§ÌîÑ Ïã§Ìå®: {e}")
        return {"status": "error", "error": str(e)}


if __name__ == "__main__":
    # Í∞úÎ∞úÏö© Ïã§Ìñâ
    celery_app.start()
